{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Загрузите данные. Используйте датасет с ирисами. Его можно загрузить непосредственно из библиотеки Sklearn. В данных оставьте только 2 класса: Iris Versicolor, Iris Virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем датасет iris, определяем признаки и целевую переменную\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оставляем 2 класса: 0 - 'versicolor', 1 - 'virginica'\n",
    "X = iris.data[iris.target != 0]\n",
    "y = iris.target[iris.target != 0] - 1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Самостоятельно реализуйте логистическую регрессию, без использования метода LogisticRegression из библиотеки. Реализуйте метод градиентного спуска. Обучите логистическую регрессию этим методом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для начала реализуем встроенный метод логистической регрессии\n",
    "\n",
    "# Разделим данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Обучим модель встроенным модулем\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# И оценим качество модели\n",
    "score = model.score(X_test, y_test)\n",
    "print(\"Оценка точности модели:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получим коэффициенты\n",
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "data_results = pd.DataFrame({\n",
    "    'Реальные значения': y_test,\n",
    "    'Предсказанные значения': y_pred\n",
    "})\n",
    "print(data_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь самостоятельно реализуем метод логистической регрессии методом градиентного спуска\n",
    "\n",
    "# Масштабируем признаки \n",
    "scaler = StandardScaler()\n",
    "X_scaled_features = scaler.fit_transform(X)\n",
    "\n",
    "# Добавляем столбец единиц для свободного веса\n",
    "X = np.c_[np.ones(len(X_scaled_features)), X_scaled_features]\n",
    "\n",
    "# Разделим данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Функция сигмоиды\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Функция для вычисления логарифмической функции потерь\n",
    "def logloss(y, y_proba):\n",
    "    return -np.mean(y * np.log(y_proba + 1e-30) + (1 - y) * np.log(1 - y_proba + 1e-30))\n",
    "\n",
    "# Градиентный спуск\n",
    "def gr_logloss(X, W, y):\n",
    "    y_proba = sigmoid(X @ W)\n",
    "    grad = X.T @ (y_proba - y) / len(y)\n",
    "    return grad\n",
    "\n",
    "# Параметры обучения \n",
    "# learning_rate: скорость обучения, epochs: количество итераций, beta: скорость сглаживания\n",
    "eps = 0.0001\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epochs = 10000\n",
    "\n",
    "# Инициализация весов\n",
    "W = np.zeros(X.shape[1])\n",
    "\n",
    "for i in range(epochs):\n",
    "    cur_W = W.copy()  # текущие веса\n",
    "    \n",
    "    # Градиент по текущим весам\n",
    "    grad = gr_logloss(X, cur_W, y)\n",
    "    \n",
    "    # Обновляем веса\n",
    "    W_next = cur_W - learning_rate * grad\n",
    "    \n",
    "    # Проверка условия остановки\n",
    "    if np.linalg.norm(W - W_next) <= eps:\n",
    "        print(f\"Остановились на итерации {i}\")\n",
    "        break\n",
    "    \n",
    "    # Обновляем веса для следующей итерации\n",
    "    W = W_next\n",
    "    \n",
    "    # Каждые 100 итераций выводим информацию\n",
    "    if i % 100 == 0:\n",
    "        y_proba = sigmoid(X @ W)\n",
    "        y_class = (y_proba >= 0.5).astype(int)\n",
    "        accuracy = (y_class == y).mean()\n",
    "        print(f\"Итерация {i}\")\n",
    "        print(f\"Logloss: {logloss(y, y_proba):.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_results = pd.DataFrame({\n",
    "    'Реальные значения': y,\n",
    "    'Предсказанные значения': y_class,\n",
    "    'Предсказанные вероятности': y_proba\n",
    "})\n",
    "print(data_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Финальные веса:\", W)\n",
    "i_grad = i\n",
    "print(\"Итераций:\", i_grad)\n",
    "accuracy_grad = accuracy\n",
    "print(\"Оценка точности модели методом градиентного спуска:\", accuracy_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Обучите логистическую регрессию этим методом скользящего среднего (Root Mean Square Propagation, RMSProp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем метод логистической регрессии методом скользящего среднего\n",
    "\n",
    "eps = 0.001\n",
    "W = np.zeros(X.shape[1])\n",
    "# Инициализация переменной для скользящего среднего квадрата градиентов\n",
    "v = np.zeros_like(W)\n",
    "\n",
    "for i in range(epochs):\n",
    "    cur_W = W.copy()  # текущие веса\n",
    "    \n",
    "    # Градиент по текущим весам\n",
    "    grad = gr_logloss(X, cur_W, y)\n",
    "    \n",
    "    # Обновляем v\n",
    "    v = beta1 * v + (1 - beta1) * (grad ** 2)\n",
    "    \n",
    "    # Обновляем веса с учетом RMSProp\n",
    "    W_next = cur_W - (learning_rate / (np.sqrt(v) + eps)) * grad\n",
    "    \n",
    "    # Проверка условия остановки\n",
    "    if np.linalg.norm(W - W_next) <= eps:\n",
    "        print(f\"Остановились на итерации {i}\")\n",
    "        break       \n",
    "    \n",
    "    # Обновляем веса для следующей итерации\n",
    "    W = W_next\n",
    "\n",
    "    # Каждые 100 итераций выводим информацию\n",
    "    if i % 100 == 0:\n",
    "        y_proba = sigmoid(X @ W)\n",
    "        y_class = (y_proba >= 0.5).astype(int)\n",
    "        accuracy = (y_class == y).mean()\n",
    "        print(f\"Итерация {i}\")\n",
    "        print(f\"Logloss: {logloss(y, y_proba):.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Финальные веса:\", W)\n",
    "i_RMSProp = i\n",
    "print(\"Итераций:\", i_RMSProp)\n",
    "accuracy_RMSProp = accuracy\n",
    "print(\"Оценка точности модели методом скользящего среднего:\", accuracy_RMSProp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Обучите логистическую регрессию этим методом адаптивной оценки моментов (Nesterov–accelerated Adaptive Moment Estimation, Nadam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем метод логистической регрессии методом адаптивной оценки моментов\n",
    "\n",
    "eps = 1e-8\n",
    "W = np.zeros(X.shape[1])\n",
    "# Инициализация переменных\n",
    "m = np.zeros_like(W)\n",
    "v = np.zeros_like(W)\n",
    "t = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    t += 1\n",
    "\n",
    "    grad = gr_logloss(X, cur_W, y)\n",
    "    \n",
    "    # Обновляем моменты\n",
    "    m = beta1 * m + (1 - beta1) * grad\n",
    "    v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "    \n",
    "    # Корректируем смещения для моментов\n",
    "    m_hat = m / (1 - beta1 ** t)\n",
    "    v_hat = v / (1 - beta2 ** t)\n",
    "    \n",
    "    # Предсказанный момент для Nesterov\n",
    "    m_bar = beta1 * m_hat + (1 - beta1) * grad / (1 - beta1 ** t)\n",
    "    \n",
    "    # Обновляем веса\n",
    "    W_next = cur_W - learning_rate * m_bar / (np.sqrt(v_hat) + eps)\n",
    "    \n",
    "    # Проверка условия остановки\n",
    "    if np.linalg.norm(W - W_next) <= eps:\n",
    "        print(f\"Остановились на итерации {i}\")\n",
    "        break       \n",
    "    \n",
    "    # Обновляем веса для следующей итерации\n",
    "    W = W_next\n",
    "\n",
    "    # Каждые 10 итераций выводим информацию\n",
    "    if i % 10 == 0:\n",
    "        y_proba = sigmoid(X @ W)\n",
    "        y_class = (y_proba >= 0.5).astype(int)\n",
    "        accuracy = (y_class == y).mean()\n",
    "        print(f\"Итерация {i}\")\n",
    "        print(f\"Logloss: {logloss(y, y_proba):.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Финальные веса:\", W)\n",
    "i_Nadam = i\n",
    "print(\"Итераций:\", i_Nadam)\n",
    "accuracy_Nadam = accuracy\n",
    "print(\"Оценка точности модели методом адаптивной оценки моментов:\", accuracy_Nadam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Сравните значение метрик для реализованных методов оптимизации. Напишите вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Метод': ['Логистическая регрессия', 'Градиентный спуск', 'Метод скользящего среднего', 'Метод Нестерова'],\n",
    "    'Метрика': [score, accuracy_grad, accuracy_RMSProp, accuracy_Nadam],\n",
    "    'Время работы (итераций)': [1, i_grad, i_RMSProp, i_Nadam]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Итоги:\n",
    "На выходе мы имеем хорошие показатели метрики оценки качества модели по всем используемым методам. Метод Нестерова показывает самый быстрый расчет по сравнению с Градиентным спуском и Скользящим средним. Наилучшие показатели точности у Метода скользящего среднего и Нестерова. Подводя итог, можно сделать вывод, что скорость и полученная метрика зависят от настроек параметров. В частности, варьируя показателями скорости обучения и количеством итераций можно повысить метрику качества для Градиентного спуска до 0.97 (learning_rate = 0.01, epochs = 100000).\n",
    "\n",
    "PS. Стоит также отметить, что в написанных функциях обучение происходит на всей выборке X (а в модуле LogisticRegression мы используем тренировочную выборку)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
